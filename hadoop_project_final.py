# -*- coding: utf-8 -*-
"""Hadoop_project_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qhjLvvSKXmPXFEvWh_jIRWCufXcpnxG4
"""

#create java home variable
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

!apt-get update
!apt-get install openjdk-8-jdk

!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz

#we’ll use the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file
!tar -xzvf hadoop-3.3.5.tar.gz

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["HADOOP_HOME"] = "/content/hadoop-3.3.5"
os.environ["PATH"] = os.environ["PATH"] + ":" + os.environ["HADOOP_HOME"] + "/bin"
os.environ["HADOOP_CONF_DIR"] = os.environ["HADOOP_HOME"] + "/etc/hadoop"

#copy  hadoop file to user/local
!cp -r hadoop-3.3.5/ /usr/local/

#To find the default Java path
!readlink -f /usr/bin/java | sed "s:bin/java::"

#Running Hadoop
!/usr/local/hadoop-3.3.5/bin/hadoop

!mkdir ~/input
!cp /usr/local/hadoop-3.3.5/etc/hadoop/*.xml ~/input

!/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep ~/input ~/grep_example 'allowed[.]*'

class Temp:
  def __init__(self,entity1, entity2):
    self.entity1=entity1
    self.entity2=entity2
  def __str__(self):
    return f"{self.entity1}\t{self.entity2}"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile mapper1.py
# # For each edge (v, u) in the graph, we emit two key-value pairs:
# # 1. The pair (v, u), which represents the directed edge from vertex v to vertex u.
# # 2. The pair (u, v), which represents the reverse direction of the edge, from vertex u to vertex v.
# import os
# import sys
# for line in sys.stdin:
#   line=line.strip()
#   key,value=line.split('\t')
#   print(f"{key}\t{value}")
#   print(f"{value}\t{key}")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile reducer1.py
# # 1. Identify the minimum value among the list of values associated with that key.
# # 2. Compare the key with the minimum value:
# #    - If the key is greater than the minimum value, we proceed to emit the key along with the minimum value.
# # 3. In addition to emitting the key and the minimum value, we also emit the pair of values present in the list and the minimum value.
# # we are using the global_var to count the number of new pairs created.
# import sys
# global_var=0
# def reducer1():
#   global global_var
#   min_value=None
#   current_key=None
#   values=[]
#   for line in sys.stdin:
#     key,value=line.strip().split('\t')
#     if  current_key == key:
#       values.append(value)
#       min_value=min(min_value,value) # finding the min value
#     else:
#       if current_key:
#         if min_value<current_key:
#           print(f"{current_key}\t{min_value}")
#           values.sort()
#           for i in values:
#             if i!=min_value:
#               global_var+=1
#               print(f"{i}\t{min_value}")
#       current_key=key
#       min_value=value
#       values=[]
#       values.append(value)
# 
#   if min_value<current_key:
#     print(f"{current_key}\t{min_value}")
#     for i in values:
#       values.sort()
#       if i!=min_value:
#         global_var+=1
#         print(f"{i}\t{min_value}")
#   with open("/content/global_var.txt","w") as f:
#     f.write(str(global_var))
# 
# reducer1()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile mapper2.py
# # This second mapper and reducer are used to eliminate any duplicate key-value pairs
# # that may have been produced during the execution of the first reducer. The goal here
# # is to ensure that we only retain unique key-value pairs after processing the intermediate results.
# 
# # In this mapper, we are utilizing a user-defined object, which is an instance of the 'Temp' class,
# # to represent the key. The value associated with each key is set to None. The 'Temp' class is designed
# # to store two entities (key and value) and allows custom comparison between pairs to ensure proper sorting.
# 
# class Temp:
#   def __init__(self,entity1, entity2):
#     self.entity1=entity1
#     self.entity2=entity2
#   def __str__(self):
#     return f"{self.entity1}\t{self.entity2}"
#   def __lt__(self,other):
#     if self.entity1==other.entity1:
#       return self.entity2<other.entity2
#     else:
#       return self.entity1<other.entity1
# import os
# import sys
# for line in sys.stdin:
#   key,value=line.strip().split('\t')
#   temp=Temp(key,value)
#   print(f"{temp}\tNone")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile reducer2.py
# # We are only trying to emit unique key value pair and are eleminating the duplicates.
# class Temp:
#   def __init__(self,entity1, entity2):
#     self.entity1=entity1
#     self.entity2=entity2
#   def __str__(self):
#     return f"{self.entity1}\t{self.entity2}"
#   def __lt__(self,other):
#     if self.entity1==other.entity1:
#       return self.entity2<other.entity2
#     else:
#       return self.entity1<other.entity1
# import os
# import sys
# current_key=None
# current_value=None
# for line in sys.stdin:
#   key,value,values=line.strip().split('\t')
#   #key, value=temp.split('\t')
#   if current_key ==key and current_value==value:
#     continue
#   else:
#     if current_key:
#       print(f"{current_key}\t{current_value}")
#     current_key=key
#     current_value=value
# 
# print(f"{current_key}\t{current_value}")
# 
# 
#

!chmod u+rwx /content/mapper1.py
!chmod u+rwx /content/reducer1.py
!chmod u+rwx /content/mapper2.py
!chmod u+rwx /content/reducer2.py

import sys
# The first set of operations involves executing the following steps:
# - mapper1 -> reducer1 -> mapper2 -> reducer2
# This chain of operations processes the input graph and sets the initial value of the global variable `global_var`.
# The purpose of `global_var` is to track whether any further changes are needed in the graph to find connected components.
# After executing the initial cycle, we proceed with checking the value of `global_var`.

# In each iteration of the while loop:
# 1. We read the value of `global_var` from the file "/content/global_var.txt".
# 2. If `global_var` equals 0, it means no changes were made in the previous iteration, indicating that all connected components have been found.
#    In this case, we break out of the loop to stop further processing.
# 3. If `global_var` is not 0, the loop continues, meaning the graph is still evolving, and we need to process another cycle.
#    In this case, we increment the iteration count, update the Hadoop streaming commands to use the appropriate input and output directories,
#    and run the next cycle of MapReduce jobs (i.e., mapper1 -> reducer1 and then mapper2 -> reducer2).
# The loop continues until `global_var == 0`, at which point the connected components are fully identified and the process terminates.

!/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar -input /input_graph.txt -output /content/output_1 -file /content/mapper1.py  -file /content/reducer1.py  -mapper 'python mapper1.py'  -reducer 'python reducer1.py'
!/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar -input /content/output_1 -output /content/output_2 -file /content/mapper2.py  -file /content/reducer2.py  -mapper 'python mapper2.py'  -reducer 'python reducer2.py'
iteration_count=0
while True:
  with open("/content/global_var.txt",'r') as f:
    global_var=int(f.read())
  if global_var==0:
    break
  iteration_count+=1
  command1 = "/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar -input /content/output_{0} -output /content/output_{1} -file /content/mapper1.py -file /content/reducer1.py -mapper 'python mapper1.py' -reducer 'python reducer1.py'".format(iteration_count, iteration_count + 1)

  command2 = "/usr/local/hadoop-3.3.5/bin/hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar -input /content/output_{0} -output /content/output_{1} -file /content/mapper2.py -file /content/reducer2.py -mapper 'python mapper2.py' -reducer 'python reducer2.py'".format(iteration_count + 1, iteration_count + 2)
  iteration_count+=1
  # Run the commands (assuming you're running in a Jupyter/Colab environment)
  !{command1}
  !{command2}